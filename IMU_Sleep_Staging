import torch
import numpy as np
import os 
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
import torchsummary
import torch.nn.functional as F
from torch.nn.utils import weight_norm
from net1d import Net1D
from torch.autograd import Variable
import gc 
from sklearn import metrics
from inception import Inception, InceptionBlock
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
from mlxtend.evaluate import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import classification_report,confusion_matrix, roc_auc_score
from sklearn import metrics

#  5: wake/ 4: rem/ 3: n1/ 2: n2/ 1: n3/ 0: unstaging
# light sleep: 4 (1)/deep sleep: 1,2,3 (2)/ wake: 5 (0)/ unstaging: 0 (3) 

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")


# K_fold and hyper parameter
k = 7; fold_num = 7;

LR = 0.00095; Batch_size = 1; # or 2
max_epoch = 100; Dropout = 0.4 

file_path = "path"

def Label_change(label): # Label for binary classification
    t_l = []
    for i in label:
        if i == 0: t_l.append(0)#(3) 
        elif i == 1: t_l.append(1) # deep sleep 
        elif i == 2: t_l.append(1) # deep sleep 
        elif i == 3: t_l.append(1) # deep sleep
        elif i == 4: t_l.append(1) # light sleep 
        else: t_l.append(0) # wake
    return t_l # change_label  

def Data_load(list_): # data load <each subject, concat>
    Data_path = "path/DATA/"
    Label_path = "path/LABEL/"
    
    Data = np.load(Data_path+ list_[0]).reshape(-1,730800,3)
    Label = np.load(Label_path+ list_[0])
    Label = np.array(Label_change(Label)).reshape(-1,812)
    for i in list_[1:]:
        pre_D = np.load(Data_path + i).reshape(-1,730800,3)
        pre_L = np.load(Label_path + i)

        pre_L = np.array(Label_change(pre_L)).reshape(-1,812)
        Label = np.concatenate((Label, pre_L), axis = 0)
        Data = np.concatenate((Data, pre_D), axis = 0)
            
    print(Data.shape, Label.shape)
    return Data, Label

def k_fold(k,fold_num,file_path): # select fold 
    subject_list = []
    for subject in sorted(os.listdir(file_path)):
        subject_list.append(subject)
    train_subject_set = []
    test_subject_set = []
    samples_in_fold = int(round(len(subject_list)/k))
    if not len(subject_list)%samples_in_fold == 0:
        supplement_num = samples_in_fold - (len(subject_list)%samples_in_fold)
        for sup in list(range(supplement_num)):
            subject_list.append(subject_list[sup])
        test_subject_set = subject_list[samples_in_fold*(fold_num-1):samples_in_fold*fold_num]
        train_subject_set = [x for x in subject_list if x not in test_subject_set]
    else:
        test_subject_set = subject_list[samples_in_fold*(fold_num-1):samples_in_fold*fold_num]
        train_subject_set = [x for x in subject_list if x not in test_subject_set]
    return train_subject_set, test_subject_set

def feature_load(list_): # feature load <each subject, concat>
    path = "path/FEATURE/"
    Data = np.load(path + list_[0]).reshape(-1,812,16)
    #print(Data.shape)
    for i in list_[1:]:
        pre_D = np.load(path + i).reshape(-1, 812, 16)    
        Data = np.concatenate((Data, pre_D), axis = 0)
    
    min_ = np.min(Data, axis = 0)
    max_ = np.max(Data, axis = 0)
    
    normalized_data = (Data - min_)/(max_ - min_)
    return normalized_data

###############################################################################
train_subject_set, test_subject_set = k_fold(k,fold_num,file_path)


data, label = Data_load(train_subject_set) 
data = data.transpose(0,2,1) 

train_x = torch.FloatTensor(data)
train_y = torch.FloatTensor(label)
print(train_x.shape, train_y.shape)

DS = TensorDataset(train_x, train_y) 
dataset = DataLoader(DS, batch_size=Batch_size) # for training
###############################################################################
test_data, test_label = Data_load(test_subject_set)

test_data = test_data.transpose(0,2,1) 
test_x = torch.FloatTensor(test_data)
test_y = torch.FloatTensor(test_label)
print(test_x.shape, test_y.shape)

t_DS = TensorDataset(test_x, test_y) 
test_dataset = DataLoader(t_DS, batch_size=Batch_size) # for test
###############################################################################

train_feature = feature_load(train_subject_set)
test_feature = feature_load(test_subject_set)

train_feature = train_feature.transpose(0,2,1)
test_feature = test_feature.transpose(0,2,1)

train_feature = torch.from_numpy(train_feature).to(device)
test_feature = torch.from_numpy(test_feature).to(device)
# =============================================================================
# Model
# =============================================================================
class Chomp1d(nn.Module):
    def __init__(self, chomp_size):
        super(Chomp1d, self).__init__()
        self.chomp_size = chomp_size
    def forward(self,x):
        return x[:, :, :-self.chomp_size].contiguous()
    
class TemporalBlock(nn.Module):
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout = Dropout):
        super(TemporalBlock, self).__init__()
        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size, 
                                           stride = stride, padding = padding, dilation = dilation))
        self.chomp1 = Chomp1d(padding)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        
        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size, 
                                           stride = stride, padding = padding, dilation = dilation))
        self.chomp2 = Chomp1d(padding)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        
        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,
                                self.conv2, self.chomp2, self.relu2, self.dropout2)
        
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        self.init_weights()
        
    def init_weights(self):
        self.conv1.weight.data.normal_(0, 0.01)
        self.conv2.weight.data.normal_(0, 0.01)
        if self.downsample is not None:
            self.downsample.weight.data.normal_(0, 0.01)
        
    def forward(self, x):
        out = self.net(x)
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)

class TemporalConvNet(nn.Module):
    def __init__(self, num_inputs, num_channels, kernel_size = 20, dropout = Dropout):
        super(TemporalConvNet, self).__init__()
        layers = []
        num_levels = len(num_channels) 
        for i in range(num_levels):
            dilation_size = 2 ** i 
            in_channels = num_inputs if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            layers += [TemporalBlock(n_inputs = in_channels, n_outputs = out_channels, 
                                     kernel_size = kernel_size, stride = 1, dilation = dilation_size, padding = (kernel_size -1 ) * dilation_size,
                                     dropout= dropout)]
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)


class TimeDistributed(nn.Module):
    def __init__(self, module, batch_first=False):
        super(TimeDistributed, self).__init__()
        self.module = module
        self.batch_first = batch_first
    def forward(self, x):
        if len(x.size()) <= 2:
            return self.module(x)
        x_reshape = x.contiguous().view(-1, x.size(-1)) 
        y = self.module(x_reshape)
        if self.batch_first:
            y = y.contiguous().view(x.size(0), -1, y.size(-1))
        else:
            y = y.view(-1, x.size(1), y.size(-1))
        return y

class ATT_inception_time(nn.Module): 
    def __init__(self, input_size, output_size):
        super(ATT_inception_time, self).__init__()
        self.causal_conv =  weight_norm(nn.Conv1d(3, 9, kernel_size=812,
                                                  stride=1, padding=811, dilation=1))
        self.chomp_att = Chomp1d(811)
        self.downsample_att = nn.Conv1d(9, 3, 1)
        self.local_att = nn.Sequential(self.causal_conv, self.chomp_att, self.downsample_att)
        self.conv1d = nn.Conv1d(3, 32, kernel_size=1, stride=1)
        self.relu = nn.ReLU()
        self.InceptionTime = nn.Sequential(
            InceptionBlock(
                in_channels=32, 
                n_filters=8, 
                kernel_sizes=[5,11,23],
                bottleneck_channels=8,
                use_residual=True,
                activation=nn.ReLU()
            ),
            InceptionBlock(
                in_channels=32, 
                n_filters=16, 
                kernel_sizes=[5,11,23],
                bottleneck_channels=16,
                use_residual=True,
                activation=nn.ReLU()
            ),
            nn.AdaptiveAvgPool1d(output_size=812)
        )
        self.conv1 = nn.Conv1d(64, 32, kernel_size=1) 
        self.tdd =TimeDistributed(nn.Linear(32,16),batch_first=True)
        # TCN
        self.tcn = TemporalConvNet(16, [128,128, 128, 128, 128, 128], kernel_size=4, dropout=0.0)        
        self.downsample = nn.Conv1d(128, 2, 1)
        
        self.downsample2 = nn.Conv1d(144, 66, 1)
        self.downsample3 = nn.Conv1d(66, 18, 1)
        self.downsample4 = nn.Conv1d(18, 1, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, inputs, feature): 
        att = self.local_att(inputs)
        for i in range(812):
            att[:,:,i*900:(i+1)*900] = F.softmax(att[:,:,i*900:(i+1)*900].clone(), dim = 2)
        
        inputs = inputs*att
        inputs = self.conv1d(att)
        inputs = self.relu(inputs)
        inception_output = self.InceptionTime(inputs)
        inception_output = self.conv1(inception_output)
        inception_output = inception_output.transpose(2, 1)
        tdd_output = self.tdd(inception_output) 
        tdd_output = tdd_output.transpose(2, 1)
        tcn_output = self.tcn(tdd_output)
        output = self.downsample(tcn_output)
        
        feature = feature.to(torch.float32)
        feature = torch.reshape(feature, (-1,16,812))
        
        pre_output = torch.cat((tcn_output, feature), dim = 1) 
        self.intermediate_output = pre_output
        output = self.downsample2(pre_output)
        output = self.downsample3(output)
        output = self.downsample4(output)
        output = self.sigmoid(output)
        print(output.shape)
        return output, att

def fit(batch_size, data, label, model, model_optimizer, feature, inference = False):
    
    loss = 0
    model_optimizer.zero_grad()
    prediction_results, att = model.forward(data, feature) 
    
    gc.collect()
    torch.cuda.empty_cache()
      
    loss_function = nn.BCELoss() # or weighted Binary crossentropy loss
    corrects = 0 
    
    label_len = []
    
    for batch in range(batch_size): 
        for i in range(812):
            if label[batch,i] == 3:
                label_len.append(i)
                break
            elif i == 811:
                label_len.append(812)
                break 
            
    for batch in range(batch_size):
        batch_prediction_results = prediction_results[batch,:,:label_len[batch]].clone()
        batch_prediction_results = batch_prediction_results.transpose(1, 0) # shape: (timesteps, 4), 4 is class
        
        batch_label = label[batch,:label_len[batch]].view([-1]).clone() # shape: (timesteps)
        current_loss = loss_function(batch_prediction_results.squeeze(), batch_label)

        loss += current_loss
        corrects_sum = ((prediction_results >= 0.5) == torch.squeeze(batch_label)).sum()
        corrects += corrects_sum
    L = sum(label_len)
    
    
    if inference is False:
        loss.backward()
        model_optimizer.step()
        
    final_loss = loss.item()
    pred_label = (prediction_results >= 0.5).long()
    pred_label = torch.squeeze(pred_label)

    gc.collect()
    torch.cuda.empty_cache()
    
    attention = torch.squeeze(att)

    return label, pred_label, prediction_results, final_loss, corrects, L, attention 

def train():
    net = ATT_inception_time(input_size = 3, output_size = 3).to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=LR)
    
    train_loss_list = []
    train_acc_list = []
    test_loss_list = []
    test_acc_list = []
    
    num_step_per_epoch_train = len(dataset)/Batch_size
    num_step_per_epoch_test = len(test_dataset)/Batch_size
    
    print("=====================================================================")
    for epoch_num in range(max_epoch):
        epoch_train_loss = 0
        train_corrects = 0
        train_L = 0
        for i, ((input_, label), t_feature) in enumerate(zip(dataset, train_feature)):
            input_ = Variable(input_).cuda().float()
            label = Variable(label).cuda().float()
            label_result, pred_label, pred, loss, batch_corrects, batch_L, attention = fit(Batch_size, input_, label, net, optimizer, t_feature)  
            
            epoch_train_loss += loss
            train_corrects += batch_corrects
            train_L += batch_L
            
            gc.collect()
            torch.cuda.empty_cache()
        
        # train
        epoch_train_loss = epoch_train_loss/num_step_per_epoch_train
        avg_train_acc = train_corrects.float()/float(train_L)
        print("epoch:{} ".format(epoch_num)+ " train loss: {}".format(epoch_train_loss))
        print("epoch:{} ".format(epoch_num)+ " train accuracy: {}".format(avg_train_acc))
        train_loss_list.append(epoch_train_loss)
        train_acc_list.append(avg_train_acc)    
        
        
        # test 
        epoch_test_loss = 0
        test_corrects = 0
        test_L = 0

        true_label = []; true_pred = []
        for i, ((input_, label), t_feature) in enumerate(zip(test_dataset, test_feature)):
            with torch.no_grad():
                input_ = Variable(input_).cuda().float()
                label = Variable(label).cuda().float()
                
                label_result, pred_label, pred, loss, batch_corrects, batch_Le, test_attention = fit(Batch_size, input_, label,net,optimizer, t_feature, inference=True)
        
            epoch_test_loss += loss
            test_corrects += batch_corrects
            test_L += batch_L
            
            label_result = label_result.long()
            true_label.extend(label_result.cpu().numpy()[0])
            true_pred.extend(pred_label.cpu().numpy())
            
            gc.collect()
            torch.cuda.empty_cache()
        
        epoch_test_loss = epoch_test_loss/num_step_per_epoch_test
        avg_test_acc = test_corrects.float()/float(test_L)
        print("epoch:{} ".format(epoch_num)+ " test loss: {}".format(epoch_test_loss))
        print("epoch:{} ".format(epoch_num)+ " test accuracy: {}".format(avg_test_acc))
        
        cohen = metrics.cohen_kappa_score(true_pred, true_label)
        f1 = f1_score(true_label, true_pred, average=None)
        print("cohen", cohen)
        print("f1 score", f1)
        print("=====================================================================")
        test_loss_list.append(epoch_test_loss)
        test_acc_list.append(avg_test_acc)
        
        path =  "./RESULT/" 

        torch.save(net.state_dict(), path + "Epoch_" + str(epoch_num) +"model.pth")
        attention = attention.cpu().data.numpy()
        
        np.save(path + "Epoch_" + str(epoch_num) +"_Attention.npy", attention)

def test():

    path = "path"
    file = path + "model.pth"
    
    net = ATT_inception_time(input_size = 3, output_size = 3).to(device)
    net.load_state_dict(torch.load(file))
    optimizer = torch.optim.Adam(net.parameters(), lr=LR)

    t_pred = []; t_label = []
    t_acc = []; 
    
    for i, ((input_, label), t_feature) in enumerate(zip(test_dataset, test_feature)):
        with torch.no_grad():
            input_ = Variable(input_).cuda().float()
            label = Variable(label).cuda().float()
            
            label_result, pred_label, pred_result, final_loss, batch_corrects, batch_L, attention = fit(Batch_size, input_, label, net, optimizer, t_feature, inference=True)
        
            acc = batch_corrects.float()/float(batch_L)
            acc = acc.cpu().numpy()
            print('accuracy', acc)
            t_acc.append(acc)
            
            t_pred.extend(pred_label.cpu().numpy())
            t_label.extend((label_result.cpu().numpy())[0])
            
            pred_label = pred_label.cpu().numpy()
            label_result = (label_result.cpu().numpy())[0]
            np.save(path + str(i) +"_label.npy", label_result)
            np.save(path + str(i) +"_pred.npy", pred_label)
            
    print("#################################################################")             
    print("Test_subject:", test_subject_set)

    t_pred = np.array(t_pred)
    t_label = np.array(t_label)
    print(t_pred.shape, t_label.shape)
    target_names = ['Wake', 'Sleep']
    print(classification_report(t_label, t_pred, digits=4, target_names = target_names))
    cm = (confusion_matrix(t_label,t_pred))
    fig, ax = plot_confusion_matrix(conf_mat=cm)
    plt.tight_layout()     
    plt.show()
    
    cohen = metrics.cohen_kappa_score(t_pred, t_label)
    print("cohen", cohen)
    f1 = f1_score(t_label, t_pred, average=None)
    print('f1_score', f1)
    auroc = roc_auc_score(t_pred, t_label)
    print("AUROC", auroc)
    
    attention = attention.cpu().data.numpy()
    print("mean_acc", np.mean(t_acc))
    print("#################################################################")
